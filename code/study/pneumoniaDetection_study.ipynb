{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Pneumonia Detection: A Neural Network Approach\n",
    "\n",
    "[Link for the Chest Pneumonia X-ray Dataset](https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Raw Data\n",
    "\t•\tInput Data: This is your initial dataset, which could be images, text, or other data types.\n",
    "\t•\tPreprocessing: Perform necessary transformations like resizing, normalization, data augmentation (in case of images), or text tokenization to prepare the data for model input.\n",
    "\n",
    "2. Training\n",
    "\t•\tTraining Set: Split the raw data into training and validation sets. The training set is used to teach the model by providing labeled data.\n",
    "\t•\tModel Training: During training, the model learns by minimizing the loss function (e.g., CrossEntropyLoss) using backpropagation and optimization (e.g., Adam optimizer).\n",
    "\t•\tEpochs: The model goes through multiple epochs, learning from the data and adjusting its weights.\n",
    "\n",
    "3. Validation\n",
    "\t•\tValidation Set: This data is used during training to evaluate the model’s performance, preventing overfitting. It is used for hyperparameter tuning and model selection (e.g., learning rate, model architecture).\n",
    "\t•\tModel Evaluation: At the end of each epoch, the model is evaluated on the validation set, and the best-performing model (lowest validation loss) is saved.\n",
    "\t•\tMetrics: You track various performance metrics like accuracy, recall, and F1 score to evaluate the model’s performance on the validation set.\n",
    "\n",
    "4. Test\n",
    "\t•\tTest Set: After training is complete, the model is tested on unseen data (test set) to evaluate its generalization ability.\n",
    "\t•\tModel Prediction: You use the model to make predictions on the test set.\n",
    "\t•\tMetrics: Evaluate the test set’s performance using metrics like accuracy, recall, F1 score, etc.\n",
    "\n",
    "5. Ensemble Prediction\n",
    "\t•\tEnsemble Models: Instead of relying on just one model, you can combine predictions from multiple models (e.g., EfficientNet-B1, B2, and B3 in your case).\n",
    "\t•\tEnsemble Strategy: Common strategies include averaging the predictions (e.g., averaging the predicted probabilities or class labels from different models).\n",
    "\t•\tFinal Prediction: The final prediction is typically a combination of multiple models’ outputs. In your case, you averaged the predictions from the three models.\n",
    "\n",
    "6. Results\n",
    "\t•\tFinal Evaluation: After combining predictions from the ensemble models, you evaluate the final predictions against the true labels from the test set.\n",
    "\t•\tFinal Metrics: You compute the final accuracy, recall, F1 score, etc., for the ensemble model.\n",
    "\t•\tInterpretation: Analyze the results to understand how well the model is performing and decide whether any improvements are needed.\n",
    "\n",
    "### Raw Data -> Preprocessing -> Train -> Validate -> Test -> Ensemble Prediction -> Final Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define preprocessing and augmentation pipelines for image data\n",
    "# Standardization: Ensure all input images have the same dimensions, pixel range, and scale. \n",
    "#                  This is crucial for models that expect a consistent input format (e.g., ResNet, EfficientNet).\n",
    "# Normalization: Normalize pixel values to have a mean of 0 and a standard deviation of 1, which helps models converge faster during training.\n",
    "# Data Augmentation (Training Only): Apply random transformations like flips, rotations, or crops to artificially expand the training dataset and improve model generalization.\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load image datasets organized in a folder hierarchy\n",
    "# ImageFolder is designed to work with image datasets stored in a folder-based structure, where each subdirectory corresponds to a specific class label. \n",
    "# It automatically: Scans the directory / Maps subdirectory names to class labels / Creates a dataset with file paths and their respective labels.\n",
    "# For custom datasets (e.g., datasets not organized into folders), you might need to write your own dataset class by subclassing torch.utils.data.Dataset.\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Handle data loading during training and evaluation\n",
    "# The DataLoader takes a dataset (like one created using ImageFolder) and provides an iterable over that dataset, with features such as:\n",
    "# 1. Batching:\n",
    "#   • Automatically divides the dataset into smaller subsets (batches) of a specified size.\n",
    "#   • Batching is critical for training neural networks efficiently, as processing data in chunks speeds up computation using GPUs.\n",
    "# 2. Shuffling:\n",
    "#   • Randomly rearranges the order of the data at the beginning of each epoch.\n",
    "#   • Shuffling prevents the model from learning unintended patterns in the order of the data (e.g., if the data is sorted by class).\n",
    "# 3. Parallel Data Loading:\n",
    "#   • Allows you to use multiple worker threads or processes to load data in parallel. This speeds up data loading for large datasets.\n",
    "# 4. Sampling:\n",
    "#   • Supports customized sampling techniques using samplers, such as randomly selecting data points or applying weighted sampling.\n",
    "# 5. Customizable Preprocessing:\n",
    "#   • Automatically applies transformations (e.g., normalization, resizing) to each data sample during loading.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Why Use EfficientNet?\n",
    "# Pre-trained models like EfficientNet are generally trained on large, publicly available datasets, with ImageNet being the most common. \n",
    "# ImageNet is a massive dataset containing millions of labeled images across 1,000 categories, such as dogs, cats, cars, etc. \n",
    "# EfficientNet, specifically, has been pre-trained on ImageNet and is therefore very good at identifying general patterns and features in images—edges, textures, shapes, etc.—that are common across a wide variety of images.\n",
    "# If you’re using EfficientNet pre-trained on ImageNet to classify chest X-ray images (a medical task), while the model has never seen chest X-ray images during pre-training, the features it learned from ImageNet (like textures, edges, etc.) are still helpful for distinguishing patterns in the X-ray images. You just need to fine-tune the model on your specific X-ray dataset.\n",
    "# Efficiency: EfficientNet models are known for being more computationally efficient and achieving higher accuracy compared to other models like ResNet and VGG. \n",
    "#             They are designed using compound scaling, which optimizes depth, width, and resolution, leading to better performance while using fewer resources.\n",
    "# Pre-trained Weights: The pre-trained models allow you to leverage transfer learning, which can greatly improve performance on your task (especially with limited data). \n",
    "#                      Instead of training a model from scratch, you can fine-tune a pre-trained EfficientNet model, saving time and computational resources.\n",
    "# Multiple Variants: EfficientNet comes in several variants (B0 to B7), with each having a different trade-off between speed and accuracy. For example:\n",
    "#   •\tB0: Smallest and fastest but less accurate.\n",
    "#   •\tB7: Largest and most accurate but slower and more resource-heavy.\n",
    "# The code above uses B1, B2, and B3, which are middle-ground options, balancing performance and computational efficiency.\n",
    "!pip install efficientnet-pytorch==0.7.1\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# The torch.nn module contains many neural network components, such as layers, loss functions, optimizers, etc.\n",
    "import torch.nn as nn\n",
    "\n",
    "# get_cosine_schedule_with_warmup is a function provided by Hugging Face’s transformers library. It’s a learning rate scheduler that adjusts the learning rate in a cosine annealing fashion.\n",
    "# Cosine annealing means the learning rate starts high, gradually decreases following a cosine curve, and then flattens out near the end of training. This schedule often helps achieve better training performance and faster convergence.\n",
    "# Cosine Annealing helps improve the performance of the model by gradually reducing the learning rate, enabling finer weight adjustments later in training.\n",
    "# Warm-up steps: The scheduler also includes “warm-up” steps, where the learning rate starts small and gradually increases to the initial learning rate over a set number of steps. This is done to avoid large updates at the start, which can destabilize the training.\n",
    "# Warm-up helps stabilize training by preventing large gradient steps at the beginning, which can cause unstable updates when weights are initialized randomly.\n",
    "# By gradually decreasing the learning rate and using warm-up, it helps the model converge more effectively and avoids oscillations.\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import accuracy_score # Function to calculate accuracy\n",
    "from sklearn.metrics import recall_score   # Function to calculate recall\n",
    "from sklearn.metrics import f1_score       # Function to calculate F1 score\n",
    "from tqdm.notebook import tqdm             # Progress bar for tracking training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import torch \n",
    "## import random\n",
    "## import numpy as np\n",
    "## import os\n",
    "\n",
    "\n",
    "## Setting the Seed\n",
    "# Defines a fixed random seed (50 in this case) for consistent results.\n",
    "seed = 50\n",
    "# Sets the seed for Python’s internal hashing functions, ensuring reproducibility in environments where Python’s hash-based operations might vary between runs.\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "# Sets the seed for Python’s built-in random module. This ensures any random operations from this module produce the same results across runs.\n",
    "random.seed(seed)\n",
    "# Sets the seed for NumPy’s random number generator. This ensures consistency in NumPy’s random operations.\n",
    "np.random.seed(seed)\n",
    "\n",
    "## Configuring PyTorch\n",
    "# Sets the seed for PyTorch’s CPU operations. Any random behavior (e.g., initialization of weights) will produce consistent results.\n",
    "torch.manual_seed(seed)\n",
    "# Sets the seed for PyTorch’s CUDA backend on a single GPU. Ensures reproducibility for GPU-based operations.\n",
    "torch.cuda.manual_seed(seed)\n",
    "# Sets the seed for all GPUs (if multiple GPUs are being used). This ensures reproducibility when using multiple devices.\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "## Configuring cuDNN\n",
    "# Forces cuDNN to use deterministic algorithms. This ensures that operations like convolution produce consistent results.\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Disables cuDNN’s auto-tuner, which selects the best convolution algorithm based on the hardware and input sizes. While this improves performance in some cases, it can introduce variability. Setting it to False prioritizes reproducibility.\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Disables cuDNN entirely. This guarantees deterministic behavior but may significantly reduce training and inference speed. This line is optional and often omitted unless required for debugging.\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locally: You could set data_path = '/path/to/your/dataset/'.\n",
    "data_path = '/kaggle/input/chest-xray-pneumonia/chest_xray/'\n",
    "\n",
    "train_path = data_path + 'train/'\n",
    "valid_path = data_path + 'val/'\n",
    "test_path = data_path + 'test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torchvision import transforms\n",
    "\n",
    "\n",
    "## transforms.Compose allows chaining multiple transformations into a single pipeline.\n",
    "# How Transforms Work\n",
    "# 1. Data Augmentation:\n",
    "#   •\tAugmentations like flipping, rotating, and cropping create diverse variations of the dataset, making the model less prone to overfitting.\n",
    "#   •\tDuring training, these augmentations are applied on-the-fly to each batch.\n",
    "# 2. Normalization:\n",
    "#   •\tBrings pixel values into a consistent range to make training stable.\n",
    "#   •\tmean and std values are specific to the dataset (ImageNet in this case).\n",
    "# 3. Pipeline:\n",
    "#   •\tEach image goes through the transformations sequentially. For example:\n",
    "#   •\tAn image is resized → cropped → augmented → converted to a tensor → normalized.\n",
    "\n",
    "## transforms.Normalize\n",
    "# Whether you can use the ImageNet normalization values (0.485, 0.456, 0.406) and (0.229, 0.224, 0.225) or need to calculate your own depends on your specific use case:\n",
    "# 1. Using ImageNet Values (0.485, 0.456, 0.406) and (0.229, 0.224, 0.225)\n",
    "#   •\tUsing a pre-trained model (e.g., ResNet, EfficientNet, etc.) from ImageNet: These models expect inputs to be normalized with these values, regardless of whether you’re training or fine-tuning on a new dataset.\n",
    "#   •\tYour dataset is similar to ImageNet: If your dataset consists of natural images (e.g., photos of animals, objects, or everyday scenes), the ImageNet values are likely a good approximation.\n",
    "# 2. Calculating Dataset-Specific Values: \n",
    "#    You should compute the mean and standard deviation for your specific dataset when:\n",
    "#   •\tYour dataset differs significantly from ImageNet: Medical imaging (e.g., X-rays, MRIs) / Grayscale or non-natural images (e.g., thermal or satellite imagery) / Cartoon, stylized, or domain-specific images.\n",
    "#   •\tYou are training a model from scratch: Without a pre-trained model, it’s better to normalize based on the statistics of your dataset to achieve better model performance.\n",
    "\n",
    "# Chain multiple transformations for training data.\n",
    "transform_train = transforms.Compose([      \n",
    "    # Resize: Ensures all images have a uniform size, regardless of their original dimensions.                                  \n",
    "    # Resize images to 250x250 pixels.\n",
    "    transforms.Resize((250, 250)),    \n",
    "    # CenterCrop: Focuses on the central part of the image, discarding less relevant peripheral areas.\n",
    "    # Crop the central 180x180 region from the resized image.      \n",
    "    transforms.CenterCrop(180),             \n",
    "    # RandomHorizontalFlip & RandomVerticalFlip: Introduces variation by flipping images, helping the model generalize better.\n",
    "    # Randomly flip the image horizontally with a 50% chance.\n",
    "    transforms.RandomHorizontalFlip(0.5),   \n",
    "    # Randomly flip the image vertically with a 20% chance.\n",
    "    transforms.RandomVerticalFlip(0.2), \n",
    "    # RandomRotation: Adds robustness by simulating images taken from different angles.    \n",
    "    # Rotate the image randomly within ±20 degrees.\n",
    "    transforms.RandomRotation(20),          \n",
    "    # ToTensor: Converts images into tensors for compatibility with PyTorch and normalizes pixel values to [0, 1].\n",
    "    # Convert the image to a PyTorch tensor and scale pixel values to [0, 1].\n",
    "    transforms.ToTensor(),                  \n",
    "    # Normalize: Standardizes image pixel values using pre-calculated ImageNet statistics for mean and std, improving convergence during training.\n",
    "    # Normalize pixel values using mean and std for each channel (RGB).\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  \n",
    "                         (0.229, 0.224, 0.225))  \n",
    "])                        \n",
    "\n",
    "# Chain transformations for test/validation data.\n",
    "transform_test = transforms.Compose([   \n",
    "    # Resize images to 250x250 pixels (same as training for consistency).\n",
    "    transforms.Resize((250, 250)),      \n",
    "    # Crop the central 180x180 region.\n",
    "    transforms.CenterCrop(180),         \n",
    "    # Convert the image to a tensor.\n",
    "    transforms.ToTensor(),              \n",
    "    # Normalize using ImageNet mean and std.\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  \n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Requirement: Your dataset folder must already be structured so that each class has its own subfolder containing all the images for that class.\n",
    "'''\n",
    "dataset/\n",
    "├── train/\n",
    "│   ├── class_1/\n",
    "│   │   ├── img1.jpg\n",
    "│   │   ├── img2.jpg\n",
    "│   ├── class_2/\n",
    "│       ├── img3.jpg\n",
    "│       ├── img4.jpg\n",
    "├── val/\n",
    "    ├── class_1/\n",
    "    │   ├── img5.jpg\n",
    "    │   ├── img6.jpg\n",
    "    ├── class_2/\n",
    "        ├── img7.jpg\n",
    "        ├── img8.jpg\n",
    "'''\n",
    "\n",
    "# root=train_path:\n",
    "#   Specifies the root directory where the training images are stored.\n",
    "#   train_path points to a folder containing subfolders, each representing a class, with images inside.\n",
    "# transform=transform_train:\n",
    "#   Applies the transformations defined in the transform_train pipeline to every image in the dataset.\n",
    "#   These transformations include resizing, flipping, rotation, and normalization, which are essential for augmenting the training data and preparing it for the model.\n",
    "datasets_train = ImageFolder(root = train_path, transform = transform_train)\n",
    "\n",
    "datasets_valid = ImageFolder(root = valid_path, transform = transform_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure reproducibility when working with PyTorch’s DataLoader in a multi-threaded or parallel environment\n",
    "\n",
    "# Ensures that each worker process in a DataLoader gets a unique but deterministic seed. \n",
    "# This avoids random operations like shuffling being inconsistent across runs.\n",
    "# worker_id: Each worker in a DataLoader is identified by a unique worker_id.\n",
    "def seed_worker(worker_id):\n",
    "    # torch.initial_seed(): Returns the initial random seed for the current worker.\n",
    "    # torch.initial_seed() % 2**32: Since torch.initial_seed() generates a large number, it’s reduced to a 32-bit range for compatibility with NumPy and Python’s random module.\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    # np.random.seed(worker_seed): Sets the seed for NumPy random number generation.\n",
    "    np.random.seed(worker_seed)\n",
    "    # random.seed(worker_seed): Sets the seed for Python’s built-in random module.\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "# Creates a random number generator (torch.Generator) specifically for controlling deterministic behavior in DataLoader sampling.\n",
    "# torch.Generator(): Creates an independent random number generator object.\n",
    "g = torch.Generator()\n",
    "# g.manual_seed(0):\n",
    "#   • Sets the seed of this generator to 0, ensuring deterministic sampling in the DataLoader.\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Defines the number of samples that will be processed together in one batch.\n",
    "# The batch size determines how many samples (images) are passed through the model at once during each training step (or iteration)\n",
    "# Smaller batches consume less memory but may increase training time, while larger batches are faster but require more memory. A batch size of 8 balances these concerns.\n",
    "batch_size = 8\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    # dataset: The dataset object you want to load (e.g., ImageFolder, custom dataset).\n",
    "    dataset = datasets_train, \n",
    "    # batch_size: Number of samples to load in each batch.\n",
    "    batch_size = batch_size, \n",
    "    # shuffle: If True, the data is shuffled at the start of each epoch.\n",
    "    shuffle = True, \n",
    "    # worker_init_fn: Allows you to set seeds for random operations in workers for reproducibility.\n",
    "    worker_init_fn = seed_worker,\n",
    "    # Provides a random number generator (torch.Generator) initialized with a fixed seed.\n",
    "    generator = g, \n",
    "    # num_workers: Number of subprocesses used for data loading. Setting this to a higher value speeds up loading but increases CPU usage.\n",
    "    # If you have more CPU cores available, you can increase the num_workers to 4 or 8. On a typical machine with 4–8 cores, setting num_workers to 4–8 should work well.\n",
    "    num_workers = 8\n",
    "    # drop_last: If True, drops the last incomplete batch if the dataset size isn’t divisible by batch_size.\n",
    ")\n",
    "\n",
    "loader_valid = DataLoader(\n",
    "    dataset = datasets_valid, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    worker_init_fn = seed_worker,\n",
    "    generator = g, \n",
    "    num_workers = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "# When to Use EfficientNet:\n",
    "#\t1.\tImage Classification Tasks:\n",
    "#\t•\tEfficientNet is particularly suited for image classification, so if your problem involves classifying images into predefined categories, EfficientNet is a good choice. It can handle tasks from binary classification (e.g., distinguishing between “healthy” and “sick” images) to multi-class classification (e.g., identifying objects or animals from a set of classes).\n",
    "#\t2.\tLimited Computational Resources:\n",
    "#\t•\tEfficientNet is known for its efficiency—it provides a good trade-off between model size and performance. If you are working with constrained resources (e.g., limited memory or computational power), EfficientNet can be a great choice because it performs well with fewer parameters and computations compared to other large CNNs (like ResNet or VGG).\n",
    "#\t3.\tTransfer Learning with Pretrained Models:\n",
    "#\t•\tEfficientNet has pretrained models on large datasets like ImageNet. If you want to take advantage of transfer learning (fine-tuning the pretrained model on your own dataset), EfficientNet is an excellent option. Transfer learning allows the model to use the features it learned on large datasets to improve its performance on your specific dataset.\n",
    "#\t4.\tLarge Datasets:\n",
    "#\t•\tIf you have a large dataset and need a model that can scale to handle increasing complexity, EfficientNet is a great choice. It scales well across multiple versions (EfficientNet-B0 to EfficientNet-B7), allowing you to select a model size that fits your dataset and computing power.\n",
    "#\t5.\tHigh Accuracy with Efficiency:\n",
    "#\t•\tEfficientNet provides high accuracy with a small number of parameters, making it ideal for situations where you need a model that performs well but doesn’t consume excessive computational resources.\n",
    "\n",
    "#When Not to Use EfficientNet:\n",
    "#\t1.\tVery Small Datasets:\n",
    "#\t•\tEfficientNet, like most deep learning models, requires a sufficient amount of data to train effectively. If you have a very small dataset, it may lead to overfitting. In such cases, you may want to consider simpler models or use techniques like data augmentation or pretraining to mitigate this.\n",
    "#\t2.\tNon-Image Data:\n",
    "#\t•\tEfficientNet is specifically designed for image data. If you’re working with non-image data (e.g., text, time series, or structured data), EfficientNet is not suitable. For text or sequence data, you would use models like LSTM, GRU, or Transformers. For structured data, simpler models like random forests or gradient boosting might be more appropriate.\n",
    "#\t3.\tReal-Time or Low-Latency Requirements:\n",
    "#\t•\tWhile EfficientNet is efficient in terms of accuracy and computation, for extremely low-latency applications (e.g., real-time video processing or embedded devices), you may need to optimize the model further or consider smaller models like MobileNet or SqueezeNet, which are specifically designed for such scenarios.\n",
    "#\t4.\tNot Suitable for Extremely High-Speed Inference Needs:\n",
    "#\t•\tAlthough EfficientNet is efficient in terms of accuracy and computational cost, for extremely high-speed inference tasks (e.g., edge devices with limited resources), it might still be too large compared to models optimized for inference at the edge. In such cases, you might prefer models optimized for inference speed, like MobileNet, ShuffleNet, or Tiny YOLO.\n",
    "#\t5.\tTasks Outside Image Classification:\n",
    "#\t•\tEfficientNet is designed primarily for image classification. If you’re working on tasks that require other types of model architectures (e.g., object detection, segmentation, or generative tasks), EfficientNet may not be the best fit, although you could adapt it as a backbone for other tasks (e.g., object detection with EfficientNet as the feature extractor).\n",
    "\n",
    "# EfficientNet.from_pretrained(): This function loads a pre-trained EfficientNet model. The model weights are pre-trained on the ImageNet dataset and can be used for transfer learning.\n",
    "# 'efficientnet-b1', 'efficientnet-b2', 'efficientnet-b3': These are specific versions of the EfficientNet model, where:\n",
    "#   •\tB1: A smaller version with fewer parameters.\n",
    "#   •\tB2 and B3: Successively larger models with more parameters and higher accuracy but also more computational requirements.\n",
    "# num_classes=2: This specifies the number of output classes for your task. In this case, it’s set to 2, which means you’re performing binary classification. For multi-class classification, you would adjust this value according to the number of classes in your dataset.\n",
    "efficientnet_b1 = EfficientNet.from_pretrained('efficientnet-b1', num_classes = 2) \n",
    "efficientnet_b2 = EfficientNet.from_pretrained('efficientnet-b2', num_classes = 2)\n",
    "efficientnet_b3 = EfficientNet.from_pretrained('efficientnet-b3', num_classes = 2) \n",
    "\n",
    "# Allocation of GPU to each model\n",
    "# .to(device): This moves the model to the specified device, either CPU or GPU. The device variable is typically set to either \"cpu\" or \"cuda\" based on whether a GPU is available for computation. This ensures the models are placed on the right hardware for training or inference.\n",
    "efficientnet_b1 = efficientnet_b1.to(device)\n",
    "efficientnet_b2 = efficientnet_b2.to(device)\n",
    "efficientnet_b3 = efficientnet_b3.to(device)\n",
    "\n",
    "# This appends the three EfficientNet models (B1, B2, B3) to a list called models_list. Storing models in a list can be useful if you want to train or evaluate multiple models in parallel or ensemble methods. \n",
    "# The list will contain the models that can later be iterated over for tasks like training, validation, or testing.\n",
    "models_list =[]\n",
    "models_list.append(efficientnet_b1)\n",
    "models_list.append(efficientnet_b2)\n",
    "models_list.append(efficientnet_b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop over a list of models (models_list) and print the number of parameters for each model\n",
    "# This line iterates over each model in the models_list using enumerate, which provides both the index (idx) and the model itself (model) in each iteration.\n",
    "for idx, model in enumerate(models_list):\n",
    "    # Calculates the total number of parameters in the model\n",
    "    #   •\tmodel.parameters() returns an iterator over all the parameters of the model (weights and biases in the layers).\n",
    "    #   •\tparam.numel() returns the total number of elements (parameters) in the given tensor param.\n",
    "    #   •\tThe sum() function sums up the number of parameters across all layers of the model.\n",
    "    num_parmas = sum(param.numel() for param in model.parameters())\n",
    "    print(f'Model{idx+1} | Number of Parameters: {num_parmas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import torch.nn as nn\n",
    "\n",
    "# nn.CrossEntropyLoss() is a loss function used for classification problems where the model outputs class probabilities, and the target labels are integers representing class indices.\n",
    "# CrossEntropyLoss is particularly useful for classification problems, and it’s most commonly applied when you have two or more mutually exclusive classes. It works well for both binary classification (two classes) and multi-class classification (more than two classes).\n",
    "\n",
    "# The loss function combines two steps:\n",
    "#   1. LogSoftmax: It applies the logarithm of softmax to the model’s output, which normalizes the output into a probability distribution.\n",
    "#   2. Negative Log Likelihood Loss: It calculates how much the predicted probabilities differ from the true class labels. The lower the loss, the better the model’s prediction matches the true labels.\n",
    "\n",
    "# If you’re working with a dataset like a binary or multi-class classification problem (for example, pneumonia vs. non-pneumonia images), and you have two classes (class_1 and class_2), this loss function would compare the model’s predicted class probabilities for each input image against the true class label (e.g., 0 or 1) and compute how far off the prediction is.\n",
    "# In this case: Class 1: Healthy (e.g., “Normal” X-ray images) / Class 2: Sick (e.g., “Pneumonia” X-ray images)\n",
    "\n",
    "# When to Use CrossEntropyLoss:\n",
    "# 1.\tClassification Tasks:\n",
    "# •\tBinary Classification: When you have two mutually exclusive classes (e.g., “positive” vs “negative”).\n",
    "# •\tMulti-Class Classification: When you have more than two classes and each sample belongs to exactly one class (e.g., classifying images into “cat,” “dog,” “bird”).\n",
    "# 2.\tSoftmax or Sigmoid Outputs:\n",
    "# •\tFor binary classification, the model’s output should be a single value (representing the logit for one class, typically “positive” or “sick”) that can be passed through a sigmoid function.\n",
    "# •\tFor multi-class classification, the model’s output should be a vector of logits (one for each class), which will be passed through a softmax function to convert the logits into probabilities.\n",
    "# 3.\tMulti-Class Problems (More than Two Classes):\n",
    "# •\tWhen you have more than two categories and each input can belong to exactly one of the categories, such as identifying animals (e.g., “dog,” “cat,” “bird”).\n",
    "\n",
    "# When Not to Use CrossEntropyLoss:\n",
    "# 1.\tMulti-Label Classification:\n",
    "# •\tWhen a sample can belong to multiple classes at once (e.g., an image can contain both a “dog” and “cat”), CrossEntropyLoss is not suitable. Instead, use a binary cross-entropy loss or sigmoid activation for each class.\n",
    "# •\tIn multi-label classification, each class is treated independently, and the model outputs a separate probability for each class.\n",
    "# 2.\tRegression Problems:\n",
    "# •\tIf your task involves predicting continuous values (e.g., predicting house prices or temperatures), CrossEntropyLoss should not be used. In such cases, you should use Mean Squared Error Loss (MSELoss) or other regression loss functions, depending on your task.\n",
    "# 3.\tOrdinal Regression:\n",
    "# •\tWhen your classes have an ordinal relationship (e.g., predicting a rating scale from 1 to 5), where the classes are ordered, but not necessarily equally spaced, you might want to consider using a loss function designed for ordinal regression, like Ordinal Cross-Entropy Loss or Mean Squared Error Loss for ordinal tasks.\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW is a variant of the Adam optimizer. It uses weight decay regularization, which helps prevent overfitting by penalizing large weights during training. \n",
    "# The “W” in AdamW stands for Weight Decay, and it decouples weight decay from the optimization process, which is often better than using the standard Adam optimizer with weight decay.\n",
    "\n",
    "optimizer1 = torch.optim.AdamW(\n",
    "    # models_list[0] refers to the first model (EfficientNet-B1) in your list of models.\n",
    "    # .parameters() retrieves the parameters (weights and biases) of the model that will be optimized.\n",
    "    models_list[0].parameters(), \n",
    "    # lr=0.0006: This sets the learning rate to 0.0006. The learning rate controls how large the steps will be when updating the model’s parameters. \n",
    "    #   A lower learning rate means smaller updates.\n",
    "    lr = 0.0006, \n",
    "    # weight_decay=0.001: This applies a weight decay of 0.001. It adds a penalty to large weights, which helps prevent overfitting and promotes generalization.\n",
    "    weight_decay = 0.001\n",
    ")\n",
    "\n",
    "optimizer2 = torch.optim.AdamW(\n",
    "    models_list[1].parameters(), \n",
    "    lr = 0.0006, \n",
    "    weight_decay = 0.001\n",
    ")\n",
    "\n",
    "optimizer3 = torch.optim.AdamW(\n",
    "    models_list[2].parameters(), \n",
    "    lr = 0.0006, \n",
    "    weight_decay = 0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "# Defines the number of epochs (full passes over the entire training dataset) for the training process. The model will train for 20 iterations over the dataset.\n",
    "epochs = 20 \n",
    "\n",
    "# his line initializes a learning rate scheduler for the first optimizer (optimizer1). The scheduler will adjust the learning rate during training according to a cosine annealing pattern, with a warm-up phase in the beginning.\n",
    "scheduler1 = get_cosine_schedule_with_warmup(\n",
    "    optimizer1, \n",
    "    # num_warmup_steps = len(loader_train) * 3:\n",
    "\t# •\tlen(loader_train) gives the number of batches in one epoch (since loader_train is the DataLoader for the training dataset).\n",
    "\t# •\t* 3: The warm-up phase will last for 3 epochs, meaning the learning rate will increase gradually from 0 to the initial learning rate during the first 3 epochs. \n",
    "    #        This helps prevent large updates at the beginning of training, which could destabilize the learning process.\n",
    "    num_warmup_steps = len(loader_train) * 3, \n",
    "    # num_training_steps = len(loader_train) * epochs:\n",
    "\t# •\tThis defines the total number of steps in the entire training process. \n",
    "    #   It is calculated as the number of batches in one epoch (len(loader_train)) multiplied by the number of epochs (epochs = 20).\n",
    "\t# •\tThis total number of steps is used to compute the cosine decay over the training process, so that the learning rate gradually decreases during the later stages of training following a cosine curve.\n",
    "    num_training_steps = len(loader_train) * epochs\n",
    ")\n",
    "\n",
    "scheduler2 = get_cosine_schedule_with_warmup(\n",
    "    optimizer2, \n",
    "    num_warmup_steps = len(loader_train) * 3, \n",
    "    num_training_steps = len(loader_train) * epochs\n",
    ")\n",
    "\n",
    "scheduler3 = get_cosine_schedule_with_warmup(\n",
    "    optimizer3, \n",
    "    num_warmup_steps = len(loader_train) * 3, \n",
    "    num_training_steps = len(loader_train) * epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score \n",
    "# from sklearn.metrics import recall_score   \n",
    "# from sklearn.metrics import f1_score       \n",
    "# from tqdm.notebook import tqdm             \n",
    "\n",
    "\n",
    "# The train function is performing the process of training and validating a deep learning model over multiple epochs, with additional functionality to save the best-performing model (based on validation loss) and evaluate it using metrics like accuracy, recall, and F1 score.\n",
    "def train(model, \n",
    "          loader_train, \n",
    "          loader_valid, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          scheduler = None, \n",
    "          epochs = 10, \n",
    "          save_file = 'model_state_dict.pth'):\n",
    "    \n",
    "    # Initialize minimum validation loss as infinity\n",
    "    # We want to track the lowest (best) validation loss across epochs during training. Initially, we don’t know what the best validation loss will be, so we set it to a very high value (infinity) to ensure that the first validation loss encountered will be smaller than this initial value.\n",
    "    valid_loss_min = np.inf \n",
    "\n",
    "    # Loop through the total number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{epochs}] \\n-----------------------------')\n",
    "        \n",
    "        ## Training\n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "        # Initialize training loss for this epoch        \n",
    "        epoch_train_loss = 0 \n",
    "        # Loop through the batches of data\n",
    "        for images, labels in tqdm(loader_train):\n",
    "            # Move images and labels to the device (GPU/CPU)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Reset gradients in the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: Get model predictions for input images\n",
    "            outputs = model(images)\n",
    "            # Calculate the loss by comparing predictions with actual labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Add current batch's loss to the total epoch loss\n",
    "            epoch_train_loss += loss.item()\n",
    "            # Backpropagation to calculate gradients \n",
    "            loss.backward()       \n",
    "            # Update model weights using gradients\n",
    "            optimizer.step()      \n",
    "            # If a scheduler is provided, update the learning rate\n",
    "            if scheduler != None: \n",
    "                scheduler.step() \n",
    "\n",
    "        # Print the average training loss for this epoch\n",
    "        print(f'\\tTraining Loss: {epoch_train_loss / len(loader_train):.4f}')\n",
    "        \n",
    "        ## Validation\n",
    "        # Set model to evaluation mode \n",
    "        model.eval()      \n",
    "        # Initialize validation loss for this epoch   \n",
    "        epoch_valid_loss = 0\n",
    "        # List to store predictions for validation \n",
    "        preds_list = []     \n",
    "        # List to store actual labels for validation\n",
    "        true_list = []       \n",
    "        \n",
    "        # Disable gradient calculation during validation\n",
    "        with torch.no_grad(): \n",
    "            for images, labels in loader_valid:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                epoch_valid_loss += loss.item()\n",
    "                \n",
    "                # Get predictions and true labels (moving to CPU for calculation)\n",
    "                preds = torch.max(outputs.cpu(), dim=1)[1].numpy() \n",
    "                true = labels.cpu().numpy() \n",
    "    \n",
    "                preds_list.extend(preds)\n",
    "                true_list.extend(true)\n",
    "                \n",
    "        # Calculate accuracy, recall, and F1 score for validation\n",
    "        val_accuracy = accuracy_score(true_list, preds_list)\n",
    "        val_recall = recall_score(true_list, preds_list)\n",
    "        val_f1_score = f1_score(true_list, preds_list)\n",
    "\n",
    "        # Print validation loss, accuracy, recall, and F1 score\n",
    "        print(f'\\tValidation Loss: {epoch_valid_loss / len(loader_valid):.4f}')\n",
    "        print(f'\\tAccuracy: {val_accuracy:.4f} / Recall: {val_recall:.4f} / F1 Score: {val_f1_score:.4f}')\n",
    "        \n",
    "        ## Finding the Optimal Model Weights\n",
    "        # When the code refers to saving the best model, it means that during training, the model is saved at the point where it performs best on the validation data—usually when the validation loss is the lowest. \n",
    "        # This is done to prevent overfitting and ensure that the model is in the best possible state for generalizing to new, unseen data.\n",
    "        # Saving the best model means the model’s parameters (weights) are stored at the epoch where it achieved the lowest validation loss or the best performance on metrics like accuracy or F1 score.\n",
    "        # If the current validation loss is lower than the previous minimum, save the model weights\n",
    "        if epoch_valid_loss <= valid_loss_min: \n",
    "            print(f'\\t### Validation Loss Decreased ({valid_loss_min:.4f} --> {epoch_valid_loss:.4f}). Saving model')\n",
    "            # Save the model's state dict (weights) to a file\n",
    "            torch.save(model.state_dict(), save_file) \n",
    "            valid_loss_min = epoch_valid_loss  # Update the minimum validation loss to the current epoch's loss\n",
    "    \n",
    "    # Return the model with the best validation loss by loading the saved weights\n",
    "    return torch.load(save_file)  # Load the saved model weights and return the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the First Model \n",
    "model_state_dict = train(\n",
    "    # model=models_list[0]: The first model in the models_list is selected, which is EfficientNet B1.\n",
    "    model = models_list[0],\n",
    "    loader_train = loader_train, \n",
    "    loader_valid = loader_valid,\n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer1,\n",
    "    scheduler = scheduler1,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "# Loading the Best Model Weights\n",
    "# This line takes the best model weights returned from the training process and loads them back into EfficientNet B1 (the first model in the list).\n",
    "# This ensures that after training, the model is set to the version that performed best on the validation set.\n",
    "models_list[0].load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = train(\n",
    "    model = models_list[1],\n",
    "    loader_train = loader_train, \n",
    "    loader_valid = loader_valid,\n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer2,\n",
    "    scheduler = scheduler2,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "models_list[1].load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = train(\n",
    "    model = models_list[2],\n",
    "    loader_train = loader_train, \n",
    "    loader_valid = loader_valid,\n",
    "    criterion = criterion, \n",
    "    optimizer = optimizer3,\n",
    "    scheduler = scheduler3,\n",
    "    epochs = epochs\n",
    ")\n",
    "\n",
    "models_list[2].load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_test: This loads the test data using the ImageFolder class. \n",
    "# It assumes the images in test_path are organized into subfolders, each corresponding to a different class. \n",
    "# The transform_test function is applied to each image to prepare them for the model (e.g., resizing, normalization).\n",
    "datasets_test = ImageFolder(root = test_path, transform = transform_test)\n",
    "\n",
    "# loader_test: This creates a DataLoader for the test dataset, specifying the batch size, number of workers for data loading, and the seed for reproducibility.\n",
    "loader_test = DataLoader(\n",
    "    dataset = datasets_test, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = False, \n",
    "    worker_init_fn = seed_worker,\n",
    "    generator = g, \n",
    "    num_workers = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose of the predict() function:\n",
    "# This function is designed to evaluate a trained model on a test dataset. \n",
    "# It makes predictions for each image in the test set, compares them to the true labels, and returns the results. \n",
    "# You can use this function to:\n",
    "# •\tGet a list of predicted labels for the test set.\n",
    "# •\tOptionally return the true labels along with the predictions for further evaluation.\n",
    "\n",
    "\n",
    "def predict(model, loader_test, return_true = False):\n",
    "    # model.eval(): This sets the model to evaluation mode, which affects layers like dropout and batch normalization. It’s important to do this before making predictions.\n",
    "    model.eval()    \n",
    "    # preds_list and true_list: These are lists to store the predicted class labels (preds_list) and the actual true labels (true_list) for the images in the test dataset.\n",
    "    # Initialize list to store predictions\n",
    "    preds_list = [] \n",
    "    # Initialize list to store true labels\n",
    "    true_list = []  \n",
    "    \n",
    "    # Disable gradient computation for predictions\n",
    "    # with torch.no_grad(): This context disables gradient calculation, which saves memory and speeds up inference since gradients are not needed for testing.\n",
    "    with torch.no_grad(): \n",
    "        # For each batch in loader_test, the images and their corresponding labels are moved to the appropriate device (GPU or CPU).\n",
    "        for images, labels in loader_test:\n",
    "            # Move images to device (GPU/CPU)\n",
    "            images = images.to(device)  \n",
    "            labels = labels.to(device)  \n",
    "            \n",
    "            # Get model's predictions\n",
    "            outputs = model(images)  \n",
    "            \n",
    "            # Get predicted class labels\n",
    "            # torch.max(outputs, dim=1): This finds the index of the maximum value in each row of the outputs tensor, which corresponds to the predicted class label.\n",
    "            # .cpu(): Ensures the data is moved back to the CPU as tensors on GPUs can't be directly converted to NumPy arrays.\n",
    "            # .numpy(): Converts the tensor to a NumPy array for easier handling.\n",
    "            preds = torch.max(outputs.cpu(), dim=1)[1].numpy()  \n",
    "            # Get true labels\n",
    "            true = labels.cpu().numpy()  \n",
    "            \n",
    "            # Add predictions to the list\n",
    "            preds_list.extend(preds)  \n",
    "            # Add true labels to the list\n",
    "            true_list.extend(true)    \n",
    "    \n",
    "    # If return_true is True, the function returns both the true labels (true_list) and the predictions (preds_list).\n",
    "    if return_true:\n",
    "        # Return both true labels and predictions\n",
    "        return true_list, preds_list  \n",
    "    else:\n",
    "        # Only return predictions\n",
    "        return preds_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_list, preds_list1 = predict(\n",
    "    model = models_list[0], \n",
    "    loader_test = loader_test, \n",
    "    return_true = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list2 = predict(\n",
    "    model = models_list[1], \n",
    "    loader_test = loader_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_list3 = predict(\n",
    "    model = models_list[2], \n",
    "    loader_test = loader_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for the efficientnet-b1 model\n",
    "print('#'*5, 'efficientnet-b1 Model Prediction Evaluation Scores', '#'*5)\n",
    "print(f'Accuracy: {accuracy_score(true_list, preds_list1):.4f}')\n",
    "print(f'Recall: {recall_score(true_list, preds_list1):.4f}')\n",
    "print(f'F1 Score: {f1_score(true_list, preds_list1):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for the efficientnet-b2 model\n",
    "print('#'*5, 'efficientnet-b2 Model Prediction Evaluation Scores', '#'*5)\n",
    "print(f'Accuracy: {accuracy_score(true_list, preds_list2):.4f}')\n",
    "print(f'Recall: {recall_score(true_list, preds_list2):.4f}')\n",
    "print(f'F1 Score: {f1_score(true_list, preds_list2):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the evaluation results for the efficientnet-b3 model\n",
    "print('#'*5, 'efficientnet-b3 Model Prediction Evaluation Scores', '#'*5)\n",
    "print(f'Accuracy: {accuracy_score(true_list, preds_list3):.4f}')\n",
    "print(f'Recall: {recall_score(true_list, preds_list3):.4f}')\n",
    "print(f'F1 Score: {f1_score(true_list, preds_list3):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why Use Ensemble?\n",
    "# Ensemble methods are used to improve the overall prediction performance. \n",
    "# By averaging the predictions of different models, you can reduce the impact of errors from any one model, leading to a more robust and accurate result. \n",
    "# In this case, you’re averaging predictions from three different models (EfficientNet-B1, EfficientNet-B2, and EfficientNet-B3). This can help mitigate the bias or variance from each individual model, leading to better generalization on the test data.\n",
    "\n",
    "ensemble_preds = []\n",
    "\n",
    "for i in range(len(preds_list1)):\n",
    "    # pred_element = np.round((preds_list1[i] + preds_list2[i] + preds_list3[i]) / 3):\n",
    "\t# •\tFor each prediction, it calculates the average of the predictions from all three models.\n",
    "\t# •\tpreds_list1[i], preds_list2[i], and preds_list3[i] are the individual predictions made by the three models (EfficientNet-B1, B2, and B3, respectively).\n",
    "\t# •\tThe sum of these predictions is divided by 3 to get the average.\n",
    "\t# •\tnp.round(...) rounds the average to the nearest integer (either 0 or 1 in the case of binary classification), creating the final ensemble prediction for that instance.\n",
    "    pred_element = np.round((preds_list1[i] + preds_list2[i] + preds_list3[i]) / 3)\n",
    "    ensemble_preds.append(pred_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#'*5, 'Final Ensemble Results Evaluation Scores', '#'*5)\n",
    "print(f'Accuracy: {accuracy_score(true_list, ensemble_preds):.4f}')\n",
    "print(f'Recall: {recall_score(true_list, ensemble_preds):.4f}')\n",
    "print(f'F1 Score: {f1_score(true_list, ensemble_preds):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
